{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aware-creek",
   "metadata": {},
   "source": [
    "# Nonlinear system identification with NARX\n",
    "\n",
    "A small case study on **nonlinear system identification** is performed in this notebook. In that context, a nonlinear autoregressive moving average model with exogenous inputs, in short **NARMAX**, is a powerful tool. It is a generalization of the famous family of **AR**/**MA**/**ARMA** time series models.\n",
    "\n",
    "At each time step $k$, the dynamical system output is denoted as $y_k$, the input is $u_k$, and $\\epsilon_k$ is the noise. The NARMAX model represents the current output as a nonlinear function $F$ of past inputs, outputs, and noise realizations:\n",
    "$$\n",
    "y_k = F(y_{k-1},\\ldots,y_{k-n_y},u_{k-1},\\ldots,u_{k-n_u},\n",
    "\\epsilon_{k-1},\\ldots,\\epsilon_{k-n_\\epsilon}) + \\epsilon_k.\n",
    "$$\n",
    "Here, $n_y$, $n_u$ and $n_\\epsilon$ are the maximum lags. It is remarked that for a linear function, an AR model could be written as $y_k = F(y_{k-1},\\ldots,y_{k-n_y}) + \\epsilon_k$, while MA would be $y_k = F(\\epsilon_{k-1},\\ldots,\\epsilon_{k-n_\\epsilon}) + \\epsilon_k$. ARMA is the straightforward combination of the previous two models.\n",
    "\n",
    "An **ARX** model has the linear structure $y_k = a_1 y_{k-1} + \\ldots + a_{n_y} y_{k-n_y} + b_1 u_{k-1} + \\ldots + b_{n_u} u_{k-n_u} + \\epsilon_k$. The coefficients are gathered in the vector $\\boldsymbol{\\beta} = (a_1,\\ldots, a_{n_y},b_1,\\ldots,b_{n_u})^\\top$. When $N$ observations are available, one can introduce $\\boldsymbol{y} = (\\ldots,y_{N-1},y_{N})^\\top$, $\\boldsymbol{\\epsilon} = (\\ldots,\\epsilon_{N-1},\\epsilon_{N})^\\top$ and\n",
    "$$\n",
    "\\boldsymbol{A} =\n",
    "\\begin{pmatrix}\n",
    "\\vdots & & \\vdots & \\vdots & & \\vdots \\\\\n",
    "y_{N-3} & \\ldots & y_{N-n_y-2} & u_{N-3} & \\ldots & u_{N-n_u-2} \\\\\n",
    "y_{N-2} & \\ldots & y_{N-n_y-1} & u_{N-2} & \\ldots & u_{N-n_u-1} \\\\\n",
    "y_{N-1} & \\ldots & y_{N-n_y} & u_{N-1} & \\ldots & u_{N-n_u}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "in order to establish the matrix form $\\boldsymbol{y} = \\boldsymbol{A} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$. The unknown coefficients are usually estimated through a **linear least squares** approach $\\hat{\\boldsymbol{\\beta}} = \\mathrm{argmin}_{\\boldsymbol{\\beta}}\\lVert \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{\\beta} \\rVert_2^2$. Many models with different regressors can be trained similarly, only MA components require some extensions.\n",
    "\n",
    "In the following, we adopt a **NARX** approach where the system inputs and outputs are related via $y_k = F(y_{k-1},\\ldots,y_{k-n_y},u_{k-1},\\ldots,u_{k-n_u}) + \\epsilon_k$. We make use of the library [SysIdentPy](https://sysidentpy.org/) and closely follow its introductory examples and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sysidentpy\n",
    "from sysidentpy.utils.generate_data import get_siso_data\n",
    "from sysidentpy.utils.display_results import results\n",
    "from sysidentpy.model_structure_selection import FROLS\n",
    "from sysidentpy.basis_function import Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e97a7f-84f8-4dd3-8e46-733ed2a8aff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13579)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-avatar",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We consider a single-input single-output (SISO) system. With the function `get_siso_data` one can simulate synthetic data. The documentation states that the simulated process is $y_k = 0.2 y_{k-1} + 0.1 y_{k-1} u_{k-1} + 0.9 u_{k-2} + \\epsilon_k$. This is a NARX-style model. Here, white noise following a Gaussian distribution $\\epsilon_k \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ is assumed and the system inputs  are independently and uniformly distributed as $u_k \\sim \\mathcal{U}(-1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, \\\n",
    "y_train, y_test = get_siso_data(\n",
    "    n=1000, # total number of samples to simulate\n",
    "    sigma=0.001, # std. of the noise-generating normal distribution\n",
    "    train_percentage=90 # percentage of train samples in the train-test split\n",
    ")\n",
    "\n",
    "print('Shape of train inputs:', X_train.shape)\n",
    "print('Shape of train outputs:', y_train.shape)\n",
    "print('Shape of test inputs:', X_test.shape)\n",
    "print('Shape of test outputs:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-identifier",
   "metadata": {},
   "source": [
    "We visualize the process realization by looking at both the simulated inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 3))\n",
    "axes[0].plot(np.arange(1, X_train.shape[0]+1), X_train, alpha=0.7)\n",
    "axes[1].plot(np.arange(1, y_train.shape[0]+1), y_train, alpha=0.7)\n",
    "axes[0].set(xlabel='time step', ylabel='value', xlim=(0, X_train.shape[0]))\n",
    "axes[1].set(xlabel='time step', ylabel='value', xlim=(0, y_train.shape[0]))\n",
    "axes[0].set_title('Inputs')\n",
    "axes[1].set_title('Outputs')\n",
    "axes[1].yaxis.set_label_position('right')\n",
    "axes[1].yaxis.tick_right()\n",
    "for ax in axes.ravel().tolist():\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-question",
   "metadata": {},
   "source": [
    "## Model fitting and selection\n",
    "\n",
    "We will use the polynomial NARX model $y_k = P_d(y_{k-1},\\ldots,y_{k-n_y},u_{k-1},\\ldots,u_{k-n_u}) + \\epsilon_k$, where $P_d$ is a polynomial of degree $d$. Fitting and selection can be performed with the class `PolynomialNarmax`. A number of hyperparameters and options have to be set beforehand. For example, we choose a second-degree polynomial with maximum lag two in both the past inputs and past outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FROLS(\n",
    "    ylag=2,                             # max. lag of the output\n",
    "    xlag=2,                             # max. lag of the input\n",
    "    order_selection=True,               # whether to use an information criterion\n",
    "    info_criteria='bic',                # information criterion for model selection\n",
    "    estimator='least_squares',          # parameter estimation method\n",
    "    basis_function=Polynomial(degree=2) # basis functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-practice",
   "metadata": {},
   "source": [
    "It is now time to perform parameter estimation and model order selection based on the training data. To that end, ordinary least squares (OLS) and the Bayesian information criterion (BIC) are used, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-myanmar",
   "metadata": {},
   "source": [
    "It is interesting to observe that the final model contains exactly the same terms as the process simulator. Moreover, the related coefficients are quite close to the actual parameter values. The estimation and selection procedures were successful in that regard. Sometimes, when a term in the selected model emerges that was not present in the original process, its estimated coefficient turns out to be comparably small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(\n",
    "    results(\n",
    "        model.final_model,\n",
    "        model.theta,\n",
    "        model.err,\n",
    "        model.n_terms,\n",
    "        err_precision=8,\n",
    "        dtype='sci'\n",
    "    ),\n",
    "    columns=['Regressor', 'Coefficient', 'Error']\n",
    ")\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-girlfriend",
   "metadata": {},
   "source": [
    "When applied to the train or test data set, our trained model yields small mean squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X=X_train, y=y_train)\n",
    "y_pred_test = model.predict(X=X_test, y=y_test)\n",
    "\n",
    "mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "mse_test = np.mean((y_test - y_pred_test)**2)\n",
    "\n",
    "print('MSE on train data: {:.4e}'.format(mse_train))\n",
    "print('MSE on test data: {:.4e}'.format(mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-modem",
   "metadata": {},
   "source": [
    "Finally, we plot the test data and the corresponding model predictions. As expected, they are in very good visual agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 3))\n",
    "axes[0].plot(np.arange(1, X_test.shape[0]+1), X_test, alpha=0.7, label='test data')\n",
    "axes[1].plot(np.arange(1, y_test.shape[0]+1), y_test, alpha=0.7, label='test data')\n",
    "axes[1].plot(np.arange(1, y_pred_test.shape[0]+1), y_test, linestyle='--', alpha=0.7, label='predictions')\n",
    "axes[0].set(xlabel='time step', ylabel='value', xlim=(0, X_test.shape[0]))\n",
    "axes[1].set(xlabel='time step', ylabel='value', xlim=(0, y_test.shape[0]))\n",
    "axes[0].set_title('Inputs')\n",
    "axes[1].set_title('Outputs')\n",
    "axes[1].yaxis.set_label_position('right')\n",
    "axes[1].yaxis.tick_right()\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[1].legend(loc='upper left')\n",
    "for ax in axes.ravel().tolist():\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

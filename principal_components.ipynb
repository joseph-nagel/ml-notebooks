{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5909e5",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "\n",
    "This notebook discusses **principal component analysis** (PCA) for dimensionality reduction. There are many great explanations of this standard technique in multivariate statistics and machine learning, including the textbook classics [[Jackson, 1991](https://doi.org/10.1002/0471725331), [Jolliffe, 2002](https://doi.org/10.1007/b98835)]. Herein we loosely follow the introduction in the last chapter of [[Nagel, 2017](https://doi.org/10.3929/ethz-a-010835772)]. After a brief theoretical summary, an experiment based on MNIST image data serves as a practical demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb940d2",
   "metadata": {},
   "source": [
    "## Population PCA\n",
    "\n",
    "We start with the **population PCA** or **discrete Karhunen-Lo√®ve expansion** of a $D$-dimensional random vector $\\boldsymbol{X}$ with mean $\\boldsymbol{\\mu} = \\mathbb{E}[\\boldsymbol{X}]$ and covariance matrix $\\boldsymbol{\\Sigma} = \\mathrm{Cov}[\\boldsymbol{X}] = \\mathbb{E}[(\\boldsymbol{X} - \\boldsymbol{\\mu})(\\boldsymbol{X} - \\boldsymbol{\\mu})^\\top]$. Because $\\boldsymbol{\\Sigma}$ is symmetric and positive definite, there are linearly independent eigenvectors $\\boldsymbol{\\phi}_i$ and positive eigenvalues $\\lambda_i$ for $i=1,\\ldots,D$ with\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} \\boldsymbol{\\phi}_i = \\lambda_i \\boldsymbol{\\phi}_i.\n",
    "$$\n",
    "It is assumed that eigenvalues are ordered such that $\\lambda_1 \\geq \\ldots \\geq \\lambda_D$. While characteristic vectors pertaining to different eigenvalues $\\lambda_i \\neq \\lambda_j$ for $i \\neq j$ are orthogonal, for degenerate values they can be easily orthogonalized. One then has $\\boldsymbol{\\phi}_i^\\top \\boldsymbol{\\phi}_j = \\delta_{ij}$ for $i,j=1,\\ldots,D$. Disregarding degeneracy, the eigenvectors are uniquely defined up to a change of sign.\n",
    "\n",
    "One can now define the orthogonal matrix $\\boldsymbol{\\Phi} = (\\boldsymbol{\\phi}_1,\\ldots \\boldsymbol{\\phi}_D)$ with $\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} = \\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^\\top = \\boldsymbol{I}$. Its columns are the eigenvectors that form a basis of the full data space. The covariance matrix is diagonalized by\n",
    "$$\n",
    "\\boldsymbol{\\Lambda} = \\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Sigma} \\boldsymbol{\\Phi} =\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\ldots & \\lambda_D \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "The other way around, one obtains the spectral eigendecomposition of the covariance matrix $\\boldsymbol{\\Sigma} = \\boldsymbol{\\Phi} \\boldsymbol{\\Lambda} \\boldsymbol{\\Phi}^\\top = \\sum_{i=1}^D \\lambda_i \\boldsymbol{\\phi}_i \\boldsymbol{\\phi}_i^\\top$. One can orthogonally transform the original random vector as follows\n",
    "$$\n",
    "\\boldsymbol{Z} = \\boldsymbol{\\Phi}^\\top (\\boldsymbol{X} - \\boldsymbol{\\mu}).\n",
    "$$\n",
    "Following this, the random vector is centered with mean $\\mathbb{E}[\\boldsymbol{Z}] = \\boldsymbol{0}$ and decorrelated with the diagonal covariance $\\mathrm{Cov}[\\boldsymbol{Z}] = \\mathbb{E}[\\boldsymbol{Z} \\boldsymbol{Z}^\\top] = \\boldsymbol{\\Lambda}$. While this implies independence in the Gaussian case, this does not generally hold true. The backtransformation is\n",
    "$$\n",
    "\\boldsymbol{X} = \\boldsymbol{\\mu} + \\boldsymbol{\\Phi} \\boldsymbol{Z} =\n",
    "\\boldsymbol{\\mu} + \\sum_{i=1}^D Z_i \\boldsymbol{\\phi}_i.\n",
    "$$\n",
    "The random variables $Z_i = \\boldsymbol{\\phi}_i^\\top (\\boldsymbol{Y} - \\boldsymbol{\\mu})$ for $i=1,\\ldots,D$ are referred to as the **principle components**. In the equation above, the original random vector has been represented in terms of those zero-mean and uncorrelated components.\n",
    "\n",
    "One can now consider the sum $\\sum_{i=1}^D \\mathrm{Var}[X_i] = \\mathrm{tr}(\\boldsymbol{\\Sigma})$ of all component variances. As a consequence of the invariance of the trace under cyclic permutation, this so-called **total variance** is preserved under the orthogonal transformation. This can be seen by writing\n",
    "$$\n",
    "\\sum_{i=1}^D \\mathrm{Var}[X_i] = \\mathrm{tr}(\\boldsymbol{\\Sigma}) =\n",
    "\\mathrm{tr}(\\boldsymbol{\\Lambda}) = \\sum_{i=1}^D \\mathrm{Var}[Z_i] = \\sum_{i=1}^D \\lambda_i.\n",
    "$$\n",
    "Recall that the eigenvalues, and thus the variances of the principal components, are arranged in decreasing order. One can truncate the series and keep only its first $D^\\prime$ terms with $1\\leq D^\\prime \\leq D$ . With $\\hat{\\boldsymbol{\\Phi}} = (\\boldsymbol{\\phi}_1,\\ldots \\boldsymbol{\\phi}_{D^\\prime})$ and $\\hat{\\boldsymbol{Z}} = \\hat{\\boldsymbol{\\Phi}}^\\top (\\boldsymbol{X} - \\boldsymbol{\\mu})$ one obtains a variance-optimal approximation or reconstruction $\\hat{\\boldsymbol{X}} = \\boldsymbol{\\mu} + \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{Z}} = \\boldsymbol{\\mu} + \\sum_{i=1}^{D^\\prime} Z_i \\boldsymbol{\\phi}_i$ of the original random vector. Most of the total variance is retained with $D^\\prime$ terms.\n",
    "\n",
    "One can see that the reconstruction error of $\\hat{\\boldsymbol{X}} = \\boldsymbol{\\mu} + \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{\\Phi}}^\\top (\\boldsymbol{X} - \\boldsymbol{\\mu})$ is given as $\\boldsymbol{X} - \\hat{\\boldsymbol{X}} = \\sum_{i=D^\\prime+1}^D Z_i \\boldsymbol{\\phi}_i$. Therefore, one has $\\lVert \\boldsymbol{X} - \\hat{\\boldsymbol{X}} \\rVert^2 = (\\boldsymbol{X} - \\hat{\\boldsymbol{X}})^\\top (\\boldsymbol{X} - \\hat{\\boldsymbol{X}}) = \\sum_{i=D^\\prime+1}^D Z_i^2$. Preserving most of the total variance corresponds to minimizing the expected squared reconstruction error. This becomes evident from\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\lVert \\boldsymbol{X} - \\hat{\\boldsymbol{X}} \\rVert^2 \\right] =\n",
    "\\sum_{i=D^\\prime+1}^D \\mathbb{E}[Z_i^2] =\n",
    "\\sum_{i=D^\\prime+1}^D \\mathrm{Var}[Z_i] = \\sum_{i=D^\\prime+1}^D \\lambda_i.\n",
    "$$\n",
    "As an alternative to the construction based on the covariance matrix eigendecomposition, it is in fact possible to define PCA as the solution to a series of optimization problems. They are related to the minimization of the expected reconstruction error $\\mathbb{E}[\\lVert \\boldsymbol{X} - \\hat{\\boldsymbol{X}} \\rVert^2] = \\mathbb{E}[\\lVert (\\boldsymbol{X} - \\boldsymbol{\\mu}) - \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{\\Phi}}^\\top (\\boldsymbol{X} - \\boldsymbol{\\mu}) \\rVert^2]$ with respect to a suitable transformation subject to $\\hat{\\boldsymbol{\\Phi}}^\\top \\hat{\\boldsymbol{\\Phi}} = \\boldsymbol{I}$.\n",
    "\n",
    "This also highlights a connection to autoencoders. The emerging latent variables of the autoencoder are neither generally ordered according to the variance nor decorrelated. For a linear encoder and decoder, that are symmetric with tied weights, and a mean squared reconstruction loss, they should span the same subspace though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9c92e",
   "metadata": {},
   "source": [
    "## Sample PCA\n",
    "\n",
    "Above, the population PCA has been applied to a random vector. Its mechanisms remain intact when being applied to an actual set of data. Instead of the exact mean and covariance matrix, their empirical counterparts enter the construction of the **sample PCA**. In particular, the sample covariance is eigendecomposed and the principal components emerge as projections on the eigenvectors.\n",
    "\n",
    "Assume that we have independent realizations $\\boldsymbol{x}^{(n)}$ for $n=1,\\ldots,N$ of our $D$-dimensional random vector. The empirical mean of each component has been subtracted, hence, all variables are already centered. As usual, the observations are then collected into the $N \\times D$-shaped data matrix\n",
    "$$\n",
    "\\mathcal{X} =\n",
    "\\begin{pmatrix}\n",
    "{\\boldsymbol{x}^{(1)}}^\\top \\\\\n",
    "{\\boldsymbol{x}^{(2)}}^\\top \\\\\n",
    "\\vdots \\\\\n",
    "{\\boldsymbol{x}^{(N)}}^\\top \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} & \\ldots & x_D^{(1)} \\\\\n",
    "x_1^{(2)} & x_2^{(2)} & \\ldots & x_D^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_1^{(N)} & x_2^{(N)} & \\ldots & x_D^{(N)}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The representation of the data items in terms of the first $D^\\prime$ principal components is given as $\\hat{\\boldsymbol{z}}^{(n)} = \\hat{\\boldsymbol{\\Phi}}^\\top \\boldsymbol{x}^{(n)}$. The correspondingly reconstructed data point is $\\hat{\\boldsymbol{x}}^{(n)} = \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{z}}^{(n)}$.\n",
    "One might summarize this by writing $\\hat{\\mathcal{Z}} = \\mathcal{X} \\hat{\\boldsymbol{\\Phi}}$ and $\\hat{\\mathcal{X}} = \\hat{\\mathcal{Z}} \\hat{\\boldsymbol{\\Phi}}^\\top$. Here, we have introduced the matrix\n",
    "$$\n",
    "\\mathcal{Z} =\n",
    "\\begin{pmatrix}\n",
    "{\\hat{\\boldsymbol{z}}^{(1)}}^\\top \\\\\n",
    "{\\hat{\\boldsymbol{z}}^{(2)}}^\\top \\\\\n",
    "\\vdots \\\\\n",
    "{\\hat{\\boldsymbol{z}}^{(N)}}^\\top \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "z_1^{(1)} & z_2^{(1)} & \\ldots & z_{D^\\prime}^{(1)} \\\\\n",
    "z_1^{(2)} & z_2^{(2)} & \\ldots & z_{D^\\prime}^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "z_1^{(N)} & z_2^{(N)} & \\ldots & z_{D^\\prime}^{(N)}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "One can now validate the optimality properties of the sample PCA. To that end, one may start by investigating the compression of $\\mathcal{X}$ into $\\mathcal{Z}$ and its subsequent reconstruction as $\\hat{\\mathcal{X}} = \\mathcal{X} \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{\\Phi}}^\\top$. The average reconstruction error can be written with the Frobenius matrix norm $\\lVert \\cdot \\rVert_F$ as\n",
    "$$\n",
    "\\frac{1}{N} \\lVert \\mathcal{X} - \\hat{\\mathcal{X}} \\rVert_F^2 =\n",
    "\\frac{1}{N} \\lVert \\mathcal{X} - \\mathcal{X} \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{\\Phi}}^\\top \\rVert_F^2.\n",
    "$$\n",
    "With $\\lVert \\mathcal{X} - \\mathcal{X} \\hat{\\boldsymbol{\\Phi}} \\hat{\\boldsymbol{\\Phi}}^\\top \\rVert_F^2 = \\mathrm{const}(\\mathcal{X}) - \\mathrm{tr}(\\hat{\\boldsymbol{\\Phi}}^\\top \\mathcal{X}^\\top \\mathcal{X} \\hat{\\boldsymbol{\\Phi}})$, where $\\boldsymbol{\\Sigma} = \\mathcal{X}^\\top \\mathcal{X} / (N-1)$ is the sample covariance of the data, one sees that the minimal reconstruction error corresponds to the maximal $\\mathrm{tr}(\\hat{\\boldsymbol{\\Phi}}^\\top \\boldsymbol{\\Sigma} \\hat{\\boldsymbol{\\Phi}})$. Here, $\\hat{\\boldsymbol{\\Phi}}^\\top \\boldsymbol{\\Sigma} \\hat{\\boldsymbol{\\Phi}}$ is exactly the empirical covariance of the transformed variables, which optimally is diagonal with decreasing entries. It is now easy to see that the sample PCA features the empirical variants of the population PCA optimalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51312eb3",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "A small demonstration of PCA applied to MNIST is provided hereafter. The implementation of the machine learning library scikit-learn is relied on. It is remarked that PyTorch and torchvision are only imported in order to ease the data loading. There is no usage of autograd or GPU computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843d361-d8d9-4f8a-aab3-bee8736b82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(666666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778247f8",
   "metadata": {},
   "source": [
    "### MNIST data\n",
    "\n",
    "The MNIST data set is imported as a first step. After this familiar-looking procedure, the grayscale images have [0,1]-valued pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.home() / 'Data'\n",
    "\n",
    "train_set = datasets.MNIST(\n",
    "    data_path,\n",
    "    train=True,\n",
    "    transform= transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    data_path,\n",
    "    train=False,\n",
    "    transform= transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print('No. train images:', len(train_set))\n",
    "print('No. test images:', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print('No. train batches:', len(train_loader))\n",
    "print('No. test batches:', len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53170a65",
   "metadata": {},
   "source": [
    "As the next step, we focus only on images showing the digit six. The images are flattened in order to obtain the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd673d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_digits(data_loader, digit):\n",
    "    '''Extract all images showing a certain digit.'''\n",
    "    images = []\n",
    "    for X_batch, y_batch in data_loader:\n",
    "        images.append(X_batch[y_batch==digit])\n",
    "    images = torch.cat(images, dim=0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = 6\n",
    "\n",
    "train_features = extract_digits(train_loader, digit).numpy().reshape((-1, 28*28))\n",
    "test_features = extract_digits(test_loader, digit).numpy().reshape((-1, 28*28))\n",
    "\n",
    "print('No. train images ({}): {}'.format(digit, len(train_features)))\n",
    "print('No. test images ({}): {}'.format(digit, len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(5, 3))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = train_features[idx].reshape((28, 28))\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(train_set.classes[digit])\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b2dc8",
   "metadata": {},
   "source": [
    "### Standard PCA\n",
    "\n",
    "A PCA involving 50 components is performed on the training set. The proportion of the preserved total variance is investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50).fit(train_features)\n",
    "print('Retained total var.: {:.2f}%'.format(100*pca.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(np.arange(pca.n_components)+1, pca.explained_variance_ratio_.cumsum(), alpha=0.7)\n",
    "ax.set(xlabel='number of principal components', ylabel='fraction of total var. retained')\n",
    "ax.set_xlim((0, pca.n_components))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275176cc",
   "metadata": {},
   "source": [
    "The empirical mean of the data is shown below. This average digit arises from reshaping the mean vector of the flattened features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "image = pca.mean_.reshape((28, 28))\n",
    "ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "ax.set_title('$\\mu$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647b7ee",
   "metadata": {},
   "source": [
    "We similarly visualize the eigenvectors pertaining to the first eight principal components. They can have positive and negative entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec87a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(5, 3))\n",
    "max_val = np.abs(pca.components_[0:8,:]).max()\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = pca.components_[idx,:].reshape((28, 28))\n",
    "    ax.imshow(image, cmap='RdGy', vmin=-max_val, vmax=max_val)\n",
    "    ax.set_title(idx+1)\n",
    "    ax.set_title('$\\phi_{}$'.format(idx+1))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00547f4",
   "metadata": {},
   "source": [
    "The test data are first reduced and subsequently reconstructed. This works well, even though the pixel value range might not be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_components = pca.transform(test_features)\n",
    "test_reconstructions = pca.inverse_transform(test_components)\n",
    "\n",
    "print('Original feature shape:', test_features.shape)\n",
    "print('Reduced dimension shape:', test_components.shape)\n",
    "print('Reconstructions shape:', test_reconstructions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = test_reconstructions[0:12,:].min()\n",
    "max_val = test_reconstructions[0:12,:].max()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(7.5, 3))\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    image = test_features[idx].reshape((28, 28))\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title('$x^{{({})}}$'.format(idx+1))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    image = test_reconstructions[idx].reshape((28, 28))\n",
    "    ax.imshow(image, cmap='gray', vmin=min_val, vmax=max_val)\n",
    "    ax.set_title('$\\\\hat{{x}}^{{({})}}$'.format(idx+1))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

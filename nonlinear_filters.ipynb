{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proprietary-hebrew",
   "metadata": {},
   "source": [
    "# Non-linear filtering\n",
    "\n",
    "We discuss and demonstrate **non-linear filtering** algorithms. They can be seen as more general alternatives to the **Kalman filter** (KF) for linear problems. In particular, the extended KF and sampling-based particle filters are studied. More detailed introductions to Bayesian filtering can be for example found in [[Särkkä, 2014](https://doi.org/10.1017/CBO9781139344203), [Candy, 2016](https://doi.org/10.1002/9780470430583)]. Among other things, mobile robot localization is an important domain of application [[Thrun et al., 2006](http://probabilistic-robotics.org)]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-frequency",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let us consider a non-linear discrete-time model, where $\\boldsymbol{x}_k$, $\\boldsymbol{u}_k$ and $\\boldsymbol{y}_k$ respectively symbolize the state, control and noise vectors at time step $k$. The system and its evolution are modeled by a state-transition and an observation equation. They are written as\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{x}_k &= f(\\boldsymbol{x}_{k-1}, \\boldsymbol{u}_k) + \\boldsymbol{w}_k, \\\\\n",
    "\\boldsymbol{y}_k &= h(\\boldsymbol{x}_k) + \\boldsymbol{v}_k.\n",
    "\\end{align*}\n",
    "$$\n",
    "Here, $f$ is a non-linear function of the state and control variables. Similarly, the function $h$ might be non-linear in the states. The discrete process noise $\\boldsymbol{w}_k \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{Q}_k)$ and the measurement noise $\\boldsymbol{v}_k \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{R}_k)$ follow Gaussian distributions.\n",
    "\n",
    "As a reminder, filtering aims at the recursive estimation of the current state $\\boldsymbol{x}_k$, given all observations $\\boldsymbol{y}_{0:k}$ up to the current step. After the initialization $\\pi(\\boldsymbol{x}_0)$, Bayesian filtering alternates between distributions of the form $\\pi(\\boldsymbol{x}_k | \\boldsymbol{y}_{0:k-1})$ and $\\pi(\\boldsymbol{x}_k | \\boldsymbol{y}_{0:k})$. They constitute the prediction and update steps\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\pi(\\boldsymbol{x}_k | \\boldsymbol{y}_{0:k-1}) &=\n",
    "\\int \\pi(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}) \\,\n",
    "\\pi(\\boldsymbol{x}_{k-1} | \\boldsymbol{y}_{0:k-1}) \\,\n",
    "\\mathrm{d} \\boldsymbol{x}_{k-1}, \\\\\n",
    "\\pi(\\boldsymbol{x}_k | \\boldsymbol{y}_{0:k}) &=\n",
    "\\frac{\\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k) \\, \\pi(\\boldsymbol{x}_k |\n",
    "\\boldsymbol{y}_{0:k-1})}{\\pi(\\boldsymbol{y}_k | \\boldsymbol{y}_{0:k-1})}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The original KF solves those general filtering equations in the linear Gaussian case. In the following, we study algorithms that address the non-linear case, where one has $\\pi(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}) = \\mathcal{N}(\\boldsymbol{x}_k | f(\\boldsymbol{x}_{k-1}, \\boldsymbol{u}_k), \\boldsymbol{Q}_k)$ and $\\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k) = \\mathcal{N}(\\boldsymbol{y}_k | h(\\boldsymbol{x}_k), \\boldsymbol{R}_k)$, but non-Gaussian filtering distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-discovery",
   "metadata": {},
   "source": [
    "## Extended Kalman filter\n",
    "\n",
    "A simple variant of the KF for weakly non-linear problems is established by the **extended Kalman filter** (EKF). It modifies the original algorithm only in that it performs linearizations about the current state estimates whenever the non-linearities cannot be handled otherwise.\n",
    "\n",
    "The state transition $\\hat{\\boldsymbol{x}}_{k|k-1} = f(\\hat{\\boldsymbol{x}}_{k-1|k-1}, \\boldsymbol{u}_k)$ and the computation of the innovation $\\boldsymbol{y}_k - h(\\hat{\\boldsymbol{x}}_{k|k-1})$ in the predict and update step, respectively, use the unaltered system functions. Other than that, one linearizes around the current estimates by reference to the partial derivatives\n",
    "$$\n",
    "\\boldsymbol{F}_k = \\left. \\frac{\\partial f}{\\partial \\boldsymbol{x}}\n",
    "\\right\\vert_{\\hat{\\boldsymbol{x}}_{k-1|k-1}, \\boldsymbol{u}_k}, \\quad\n",
    "\\boldsymbol{H}_k = \\left. \\frac{\\partial h}{\\partial \\boldsymbol{x}}\n",
    "\\right\\vert_{\\hat{\\boldsymbol{x}}_{k|k-1}}.\n",
    "$$\n",
    "\n",
    "Following the KF algorithm, initialized with $\\hat{\\boldsymbol{x}}_{0|0}$, $\\boldsymbol{P}_{0|0}$, one proceeds in two alternating steps. The first EKF step utilizes the exact transition equation for the state estimate, but linearly approximates the propagation of its covariance matrix. This predict step can be written as\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{x}}_{k|k-1} &=\n",
    "f(\\hat{\\boldsymbol{x}}_{k-1|k-1}, \\boldsymbol{u}_k), \\\\\n",
    "\\boldsymbol{P}_{k|k-1} &=\n",
    "\\boldsymbol{F}_k \\boldsymbol{P}_{k-1|k-1} \\boldsymbol{F}_k^\\top +\n",
    "\\boldsymbol{Q}_k.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the second step of the EKF one starts with computing the innovation vector $\\boldsymbol{y}_k - h(\\hat{\\boldsymbol{x}}_{k|k-1})$. Linear approximations are used for its covariance matrix $\\boldsymbol{S}_k = \\boldsymbol{H}_k \\boldsymbol{P}_{k|k-1} \\boldsymbol{H}_k^\\top + \\boldsymbol{R}_k$ and the near-optimal Kalman gain $\\boldsymbol{K}_k = \\boldsymbol{P}_{k|k-1} \\boldsymbol{H}_k^\\top \\boldsymbol{S}_k^{-1}$. Altogether the update step is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{x}}_{k|k} &=\n",
    "\\hat{\\boldsymbol{x}}_{k|k-1} +\n",
    "\\boldsymbol{K}_k (\\boldsymbol{y}_k - h(\\hat{\\boldsymbol{x}}_{k|k-1})), \\\\\n",
    "\\boldsymbol{P}_{k|k} &=\n",
    "\\left( \\boldsymbol{I} - \\boldsymbol{K}_k \\boldsymbol{H}_k \\right) \\boldsymbol{P}_{k|k-1}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The EKF approximates the general filtering distributions as Gaussians. They are constructed based on the discussed linearizations. In doing so, the strict optimality properties of the KF are unfortunately lost. Moreover, the EKF tends to underestimate the uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-musical",
   "metadata": {},
   "source": [
    "## Particle filters\n",
    "\n",
    "The family of **particle filter** (PF) algorithms utilize sampling-based approximations of the Bayesian filtering equations. They work for non-linear models and avoid simplifying Gaussian assumptions or approximations. Such techniques are also referred to as **sequential Monte Carlo** (SMC). We start with an introduction to computing posterior expectations via Monte Carlo and importance sampling. Several PF schemes are subsequently introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-albuquerque",
   "metadata": {},
   "source": [
    "### Importance sampling\n",
    "\n",
    "Let us consider a general posterior $\\pi(\\boldsymbol{x} | \\boldsymbol{y}) = Z^{-1} \\pi(\\boldsymbol{y} | \\boldsymbol{x}) \\pi(\\boldsymbol{x})$ with $Z = \\int \\pi(\\boldsymbol{y} | \\boldsymbol{x}) \\pi(\\boldsymbol{x}) \\, \\mathrm{d} \\boldsymbol{x}$. If one could draw $N$ random samples $\\boldsymbol{x}^{(i)} \\sim \\pi(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})$ for $i=1,\\ldots,N$ independently from this distribution, expected values would be approximated via **Monte Carlo** (MC) simulation\n",
    "$$\n",
    "\\mathbb{E}[g(\\boldsymbol{x}) | \\boldsymbol{y}] =\n",
    "\\int g(\\boldsymbol{x}) \\, \\pi(\\boldsymbol{x} | \\boldsymbol{y}) \\, \\mathrm{d} \\boldsymbol{x} \\approx\n",
    "\\frac{1}{N} \\sum_{i=1}^N g(\\boldsymbol{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "Normally, however, the target cannot be sampled easily. Instead, if independent samples could be generated from an auxiliary distribution $\\boldsymbol{x}^{(i)} \\sim q(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})$ with greater or equal support, then the posterior expectations could be approximated by **importance sampling** (IS) as follows\n",
    "$$\n",
    "\\mathbb{E}[g(\\boldsymbol{x}) | \\boldsymbol{y}] =\n",
    "\\int g(\\boldsymbol{x}) \\frac{\\pi(\\boldsymbol{x} | \\boldsymbol{y})}{q(\\boldsymbol{x} | \\boldsymbol{y})}\n",
    "q(\\boldsymbol{x} | \\boldsymbol{y}) \\, \\mathrm{d} \\boldsymbol{x} \\approx\n",
    "\\sum_{i=1}^N \\tilde{w}^{(i)} g(\\boldsymbol{x}^{(i)}), \\quad\n",
    "\\tilde{w}^{(i)} = \\frac{1}{N}\n",
    "\\frac{\\pi(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})}{q(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})}.\n",
    "$$\n",
    "\n",
    "This is still of little practical use unless the posterior density $\\pi(\\boldsymbol{x} | \\boldsymbol{y})$ can be evaluated analytically. As a remedy, one can apply the IS re-weighting simultaneously to the numerator and denominator in order to obtain\n",
    "$$\n",
    "\\mathbb{E}[g(\\boldsymbol{x}) | \\boldsymbol{y}] =\n",
    "\\frac{\\int g(\\boldsymbol{x}) \\, \\pi(\\boldsymbol{y} | \\boldsymbol{x}) \\, \\pi(\\boldsymbol{x}) \\, \\mathrm{d} \\boldsymbol{x}}{\\int \\pi(\\boldsymbol{y} | \\boldsymbol{x}) \\, \\pi(\\boldsymbol{x}) \\, \\mathrm{d} \\boldsymbol{x}} =\n",
    "\\frac{\\int g(\\boldsymbol{x}) \\frac{\\pi(\\boldsymbol{y} | \\boldsymbol{x}) \\pi(\\boldsymbol{x})}{q(\\boldsymbol{x} | \\boldsymbol{y})} q(\\boldsymbol{x} | \\boldsymbol{y}) \\, \\mathrm{d} \\boldsymbol{x}}{\\int \\frac{\\pi(\\boldsymbol{y} | \\boldsymbol{x}) \\pi(\\boldsymbol{x})}{q(\\boldsymbol{x} | \\boldsymbol{y})} q(\\boldsymbol{x} | \\boldsymbol{y}) \\, \\mathrm{d} \\boldsymbol{x}}.\n",
    "$$\n",
    "Posterior expected values are then approximated with the generated samples $\\boldsymbol{x}^{(i)}$ for $i=1,\\ldots,N$ and the corresponding evaluations of the likelihood $\\pi(\\boldsymbol{y} | \\boldsymbol{x}^{(i)})$, the prior density $\\pi(\\boldsymbol{x}^{(i)})$ and the auxiliary density $q(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})$. This is written as\n",
    "$$\n",
    "\\mathbb{E}[g(\\boldsymbol{x}) | \\boldsymbol{y}] \\approx\n",
    "\\sum_{i=1}^N w^{(i)} g(\\boldsymbol{x}^{(i)}), \\quad\n",
    "w^{(i)} = \\frac{\\pi(\\boldsymbol{y} | \\boldsymbol{x}^{(i)}) \\, \\pi(\\boldsymbol{x}^{(i)})}{q(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})}\n",
    "\\left( \\sum_{j=1}^N \\frac{\\pi(\\boldsymbol{y} | \\boldsymbol{x}^{(j)}) \\, \\pi(\\boldsymbol{x}^{(j)})}{q(\\boldsymbol{x}^{(j)} | \\boldsymbol{y})} \\right)^{-1}.\n",
    "$$\n",
    "That the importance weights $w^{(i)}$ are normalized versions of $\\pi(\\boldsymbol{y} | \\boldsymbol{x}^{(i)}) \\pi(\\boldsymbol{x}^{(i)}) / q(\\boldsymbol{x}^{(i)} | \\boldsymbol{y})$ facilitates their practical computation. An IS-based representation of the posterior density can be formally written as $\\pi(\\boldsymbol{x} | \\boldsymbol{y}) \\approx \\sum_{i=1}^N w^{(i)} \\delta(\\boldsymbol{x} - \\boldsymbol{x}^{(i)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-booth",
   "metadata": {},
   "source": [
    "### Sequential importance sampling\n",
    "\n",
    "Similar to described above, one can proceed for a posterior of the form $\\pi(\\boldsymbol{x}_{0:k} | \\boldsymbol{y}_{0:k}) = Z_{0:k}^{-1} \\pi(\\boldsymbol{x}_0) \\prod_{i=0}^k \\pi(\\boldsymbol{y}_i | \\boldsymbol{x}_i) \\prod_{i=1}^k \\pi(\\boldsymbol{x}_i | \\boldsymbol{x}_{i-1})$. As a consequence of the Markovian model, the posterior $\\pi(\\boldsymbol{x}_{0:k} | \\boldsymbol{y}_{0:k})$ at time step $k$ is recursively related to its predecessor $\\pi(\\boldsymbol{x}_{0:k-1} | \\boldsymbol{y}_{0:k-1})$ at $k-1$ through\n",
    "$$\n",
    "\\pi(\\boldsymbol{x}_{0:k} | \\boldsymbol{y}_{0:k}) =\n",
    "\\frac{Z_{0:k-1}}{Z_{0:k}}\n",
    "\\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k) \\,\n",
    "\\pi(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}) \\,\n",
    "\\pi(\\boldsymbol{x}_{0:k-1} | \\boldsymbol{y}_{0:k-1}).\n",
    "$$\n",
    "A recursive IS scheme can then be devised based on draws from an auxiliary distribution $\\boldsymbol{x}_{0:k}^{(i)} \\sim q(\\boldsymbol{x}_{0:k}^{(i)} | \\boldsymbol{y}_{0:k})$ for $i=1,\\ldots,N$. In doing so, if the latter density is built in sequential fashion $q(\\boldsymbol{x}_{0:k} | \\boldsymbol{y}_{0:k}) = q(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}, \\boldsymbol{y}_{0:k}) q(\\boldsymbol{x}_{0:k-1} | \\boldsymbol{y}_{0:k-1})$, one obtains a recursion for the weights\n",
    "$$\n",
    "w_k^{(i)} \\propto \\frac{\\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k^{(i)}) \\, \\pi(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)}) \\, \\pi(\\boldsymbol{x}_{0:k-1}^{(i)} | \\boldsymbol{y}_{0:k-1})}{q(\\boldsymbol{x}_{0:k}^{(i)} | \\boldsymbol{y}_{0:k})} =\n",
    "\\frac{\\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k^{(i)}) \\, \\pi(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)})}{q(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)}, \\boldsymbol{y}_{0:k})}\n",
    "\\underbrace{\\frac{\\pi(\\boldsymbol{x}_{0:k-1}^{(i)} | \\boldsymbol{y}_{0:k-1})}{q(\\boldsymbol{x}_{0:k-1}^{(i)} | \\boldsymbol{y}_{0:k-1})}}_{\\propto w_{k-1}^{(i)}}.\n",
    "$$\n",
    "Beyond that, because of the recursive structure of the auxiliary distribution, samples $\\boldsymbol{x}_k^{(i)}$ can be randomly generated by $\\boldsymbol{x}_k^{(i)} \\sim q(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)}, \\boldsymbol{y}_{0:k})$ from samples $\\boldsymbol{x}_{k-1}^{(i)}$ at the previous step.\n",
    "\n",
    "Based on this sampling strategy, expectations under the full posterior are simulated as $\\mathbb{E}[g(\\boldsymbol{x}_{0:k}) | \\boldsymbol{y}_{0:k}] \\approx \\sum_{i=1}^N w_k^{(i)} g(\\boldsymbol{x}_{0:k}^{(i)})$. That means that the posterior density is formally approximated as $\\pi(\\boldsymbol{x}_{0:k} | \\boldsymbol{y}_{0:k}) \\approx \\sum_{i=1}^N w_k^{(i)} \\delta(\\boldsymbol{x}_{0:k} - \\boldsymbol{x}_{0:k}^{(i)})$. The filtering distribution $\\pi(\\boldsymbol{x}_k | \\boldsymbol{y}_{0:k}) = \\int \\pi(\\boldsymbol{x}_{0:k} | \\boldsymbol{y}_{0:k}) \\, \\mathrm{d} \\boldsymbol{x}_{0:k-1}$ is a marginal of the full posterior. It can be approximated by simply disregarding the past $\\boldsymbol{x}_{0:k-1}^{(i)}$ and focussing on the current $\\boldsymbol{x}_k^{(i)}$. One then has\n",
    "$$\n",
    "\\mathbb{E}[g(\\boldsymbol{x}_k) | \\boldsymbol{y}_{0:k}] \\approx\n",
    "\\sum_{i=1}^N w_k^{(i)} g(\\boldsymbol{x}_k^{(i)})\n",
    "$$\n",
    "for the expectations and $\\pi(\\boldsymbol{x}_k | \\boldsymbol{y}_{0:k}) \\approx \\sum_{i=1}^N w_k^{(i)} \\delta(\\boldsymbol{x}_k - \\boldsymbol{x}_k^{(i)})$ for the density. In this context, $(\\boldsymbol{x}_k^{(i)}, w_k^{(i)})$ for $i=1,\\ldots,N$ are called **particles**. They are just importance-weighted samples from the auxiliary distribution.\n",
    "\n",
    "The **sequential importance sampling** (SIS) scheme is now summarized. One starts by drawing $N$ samples from the prior distribution $\\boldsymbol{x}_0^{(i)} \\sim \\pi(\\boldsymbol{x}_0^{(i)})$ and by setting the initial weights to $w_0^{(i)} = 1/N$ for $i=1,\\ldots,N$. One iterates as follows after this initialization. In each step $k$ one generates new samples $\\boldsymbol{x}_k^{(i)} \\sim q(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)}, \\boldsymbol{y}_{0:k})$ from the old ones $\\boldsymbol{x}_{k-1}^{(i)}$. Similarly, the corresponding weights $w_k^{(i)} \\propto w_{k-1}^{(i)} \\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k^{(i)}) \\pi(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)}) / q(\\boldsymbol{x}_k^{(i)} | \\boldsymbol{x}_{k-1}^{(i)}, \\boldsymbol{y}_{0:k})$ are first computed from their old values $w_{k-1}^{(i)}$. Thereafter, they are normalized such that they sum to one.\n",
    "\n",
    "A practial difficulty is the selection of an appropriate importance density. Optimally, $q(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}, \\boldsymbol{y}_{0:k}) = \\pi(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}, \\boldsymbol{y}_k) \\propto \\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k) \\pi(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1})$, but this is usually not feasible. If it would be possible, one could in fact sample the posterior sequentially and directly, which in turn would render the whole SIS redundant. It is remarked that for such a hypothetical choice, the importance weights $w_k^{(i)} = w_{k-1}^{(i)}$ would remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-participant",
   "metadata": {},
   "source": [
    "### Sequential importance resampling\n",
    "\n",
    "SIS particle filtering via suffers from a well-known degeneracy problem. It refers to the situation that after a couple of algorithmic iterations, all but very few particles have negligible weights. This limits how well the posterior distribution can be represented through the particles.\n",
    "\n",
    "The **sequential importance resampling** (SIR) scheme introduces an additional resampling step in order to overcome this problem. At the end of an iteration, the particles are bootstrapped, that means randomly sampled with replacement. Their current weights are used as the discrete probabilities $P(\\boldsymbol{x}_{k,\\mathrm{new}}^{(j)} = \\boldsymbol{x}_k^{(i)}) = w_k^{(i)}$ for $j=1,\\ldots,N$ of a categorical distribution. Subsequently, the weights are uniformly reset to $w_{k,\\mathrm{new}}^{(j)} = 1/N$. Such a resampling can be done at regular intervals or when the effective particle number drops below a certain fraction of the total number.\n",
    "\n",
    "A simple SIR variant is the **bootstrap filter**. It owes its name to the fact that it resamples at the end of each iteration. In addition, it merely employs the state transition prior as an importance density $q(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1}, \\boldsymbol{y}_{0:k}) = \\pi(\\boldsymbol{x}_k | \\boldsymbol{x}_{k-1})$. The weight update reduces to $w_k^{(i)} \\propto w_{k-1}^{(i)} \\pi(\\boldsymbol{y}_k | \\boldsymbol{x}_k^{(i)})$ for such a data-independent construction. This is appealing for its simplicity and conceptionally resembles the two-step predict/update approach of the KF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-plumbing",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "We now consider a noisy gravity pendulum which is frequently discussed in [[Särkkä, 2014](https://doi.org/10.1017/CBO9781139344203), [Särkkä and Solin, 2019](https://doi.org/10.1017/9781108186735)]. This is one of the simplest but useful non-linear examples. In terms of the non-linearities, it is reminiscent of widely used localization task models, for example in robotics. After the model specification, an experiment with simulated data is conducted, where the stochastic system is state-estimated via EKF and PF algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958cb88-2001-44a4-9520-23bba7189afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-fruit",
   "metadata": {},
   "source": [
    "The motion of a **simple pendulum** can be described by the ordinary differential equation $\\ddot{\\theta} = -g/l \\sin(\\theta)$, where $\\theta$ is the angular displacement. The gravitational acceleration is denoted as $g$ and the length is $l$. Notice that the small-angle approximation $\\sin(\\theta) \\approx \\theta$ would yield a harmonic oscillator. If one additionally assumes that randomly acting forces cause acceleration changes, one obtains the model of a **stochastic pendulum**\n",
    "$$\n",
    "\\frac{\\mathrm{d}^2 \\theta}{\\mathrm{d} t^2} = -\\frac{g}{l} \\sin(\\theta) + w(t).\n",
    "$$\n",
    "Here, $w(t)$ is a continuous-time Gaussian white noise process with $\\mathbb{E}[w(t)] = 0$ and $\\mathbb{E}[w(t) w(\\tau)] = \\sigma_w^2 \\delta(t-\\tau)$. One can define the state variables $\\boldsymbol{x} = (x_1, x_2)^\\top = (\\theta, \\dot{\\theta})^\\top$ in order to write the model in the non-linear continuous-time state-space form\n",
    "$$\n",
    "\\begin{pmatrix} \\dot{x}_1 \\\\ \\dot{x}_2 \\end{pmatrix} =\n",
    "\\begin{pmatrix} x_2 \\\\ -\\frac{g}{l} \\sin(x_1) \\end{pmatrix} +\n",
    "\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} w(t).\n",
    "$$\n",
    "\n",
    "It is noted that the non-linearity has emerged due to simple trigonometry. As a matter of fact, this analogously happens for many other geometry-related models involving angles. One can derive a discretized and approximate but non-linear state-space model\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix}}_{\\boldsymbol{x}_k} =\n",
    "\\underbrace{\\begin{pmatrix}\n",
    "x_{1,k-1} + x_{2,k-1} \\Delta t \\\\\n",
    "x_{2,k-1} -\\frac{g}{l} \\sin(x_{1,k-1}) \\Delta t\n",
    "\\end{pmatrix}}_{f(\\boldsymbol{x}_{k-1})} +\n",
    "\\underbrace{\\begin{pmatrix} w_{1,k} \\\\ w_{2,k} \\end{pmatrix}}_{\\boldsymbol{w}_k}, \\quad\n",
    "\\boldsymbol{Q} = \\sigma_w^2\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{3} (\\Delta t)^3 & \\frac{1}{2} (\\Delta t)^2 \\\\\n",
    "\\frac{1}{2} (\\Delta t)^2 & \\Delta t\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "The discrete-time noise random variable $\\boldsymbol{w}_k = (w_{1,k}, w_{2,k})^\\top \\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{Q})$ follows a Gaussian distribution with covariance $\\boldsymbol{Q}$. This result ensues from a small-angle simplification of the continuous equations and a first-order time-discretization of the solution.\n",
    "\n",
    "A common scenario is that angular displacements can be measured subject to normally distributed noise. The observation model completes the problem setup. For the sake of completeness, it is written as\n",
    "$$\n",
    "y_k = x_{1,k} + v_k, \\quad v_k \\sim \\mathcal{N}(0, \\sigma_v^2).\n",
    "$$\n",
    "\n",
    "It is recalled that the EKF demands linearizations of both the transition and observation model at the current state estimates. Because our measurement model is already linear, we only have to linearize the discrete-time transition model. This is accomplished by means of the Jacobian matrix\n",
    "$$\n",
    "\\boldsymbol{F}_k = \\boldsymbol{J}_f(\\boldsymbol{x}_{k-1}) =\n",
    "\\left. \\frac{\\partial f}{\\partial \\boldsymbol{x}}\n",
    "\\right\\vert_{\\boldsymbol{x}_{k-1}} =\n",
    "\\begin{pmatrix} 1 & \\Delta t \\\\ -\\frac{g}{l} \\cos(x_{1,k-1}) \\Delta t & 1 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Let us shortly investigate how this relates to the predict step. Provided that $f(\\boldsymbol{x})$ is a function of some random variable $\\boldsymbol{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, the first-order Taylor expansion about the mean is $f(\\boldsymbol{x}) = f(\\boldsymbol{\\mu} + \\Delta \\boldsymbol{x}) \\approx f(\\boldsymbol{\\mu}) + \\boldsymbol{J}_f(\\boldsymbol{\\mu}) \\Delta \\boldsymbol{x}$. Consequentially, the linear approximations of the transformed mean and covariance are $\\mathbb{E}[f(\\boldsymbol{x})] \\approx f(\\boldsymbol{\\mu})$ and $\\mathrm{Cov}[f(\\boldsymbol{x})] \\approx \\boldsymbol{J}_f(\\boldsymbol{\\mu}) \\boldsymbol{\\Sigma} \\boldsymbol{J}_f(\\boldsymbol{\\mu})^\\top$, respectively, which exactly corresponds to the EKF predict step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-driver",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "We start with a simulator of the stochastic pendulum as discussed above. It randomly generates sample paths based on the discrete model. A symplectic leapfrog integrator, which enhances stability due to conserving an approximate energy, is employed for the time stepping. The simplest such method performs only one time step: a half step for the positions, followed by a full step for the velocities, completed by a half position step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticPendulum():\n",
    "    '''Simulator of the stochastic pendulum.'''\n",
    "\n",
    "    def __init__(self, g, l, dt, sigma_w, sigma_v, x0=None):\n",
    "        self.g = g\n",
    "        self.l = l\n",
    "        self.dt = dt\n",
    "\n",
    "        self.process_noise = norm(scale=sigma_w * np.sqrt(dt))\n",
    "        self.measure_noise = norm(scale=sigma_v)\n",
    "\n",
    "        if x0 is None:\n",
    "            self.x = np.zeros((2,))\n",
    "        else:\n",
    "            self.x = np.array(x0).reshape((2,)).astype('float')\n",
    "\n",
    "    def evolve(self):\n",
    "        self.x[0] += 0.5 * self.x[1] * self.dt\n",
    "        self.x[1] += -self.g / self.l * np.sin(self.x[0]) * self.dt\n",
    "        self.x[1] += self.process_noise.rvs()\n",
    "        self.x[0] += 0.5 * self.x[1] * self.dt\n",
    "\n",
    "    def observe(self):\n",
    "        return self.x[0] + self.measure_noise.rvs()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 10 # gravitational acceleration\n",
    "l = 0.1 # pendulum length\n",
    "dt = 0.01 # time step\n",
    "sigma_w = 1 # process noise intensity\n",
    "sigma_v = 0.05 # measurement noise level\n",
    "x0 = (0.2, 0) # initial state\n",
    "\n",
    "real_system = StochasticPendulum(g, l, dt, sigma_w, sigma_v, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 300\n",
    "\n",
    "x_true = np.zeros((num_steps+1, 2))\n",
    "y_meas = np.zeros(num_steps)\n",
    "\n",
    "x_true[0] = real_system.get_state()\n",
    "for k in range(num_steps):\n",
    "    real_system.evolve()\n",
    "    x_true[k+1] = real_system.get_state()\n",
    "    y_meas[k] = real_system.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 4))\n",
    "axes[0].plot(np.arange(num_steps+1), x_true[:,0], color=plt.cm.Dark2(2), zorder=3, label='true process')\n",
    "axes[0].plot(np.arange(1, num_steps+1), y_meas, linestyle='', marker='o', markersize=5,\n",
    "             color=plt.cm.Dark2(1), alpha=0.2, clip_on=False, zorder=2, label='observations')\n",
    "axes[0].set(xlabel='time step', ylabel='angular position', xlim=(0, num_steps))\n",
    "axes[1].plot(np.arange(num_steps+1), x_true[:,1], color=plt.cm.Dark2(2), zorder=3, label='true process')\n",
    "axes[1].set(xlabel='time step', ylabel='angular velocity', xlim=(0, num_steps))\n",
    "for ax in axes.ravel().tolist():\n",
    "    ax.legend()\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "fig.suptitle('stochastic pendulum simulation')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-calgary",
   "metadata": {},
   "source": [
    "### Extended Kalman filter\n",
    "\n",
    "The EKF algorithm is now applied in order to state-estimate the stochastically driven pendulum in real-time. In the present case, the observation model is already linear and therefore does not require special attention. Only the transition model has to be linearized about the current state estimates. Eventually, we can compare the estimated states and their uncertainties against the actually realized values of the simulated process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedKalmanFilter():\n",
    "    '''Naive implementation of the extended Kalman filter.'''\n",
    "\n",
    "    def __init__(self, g, l, dt,\n",
    "                 sigma_w, sigma_v,\n",
    "                 x0=None, P0=None):\n",
    "        self.g = g\n",
    "        self.l = l\n",
    "        self.dt = dt\n",
    "\n",
    "        self.Q = np.array([[(dt**3)/3, (dt**2)/2],\n",
    "                           [(dt**2)/2, dt]]) * sigma_w**2\n",
    "\n",
    "        self.H = np.array([[1., 0.]])\n",
    "\n",
    "        self.R = sigma_v**2\n",
    "\n",
    "        if x0 is None:\n",
    "            self.x = np.zeros((2, 1))\n",
    "        else:\n",
    "            self.x = np.array(x0).reshape((2, 1)).astype('float')\n",
    "\n",
    "        if P0 is None:\n",
    "            self.P = np.eye(2)\n",
    "        else:\n",
    "            self.P = np.array(P0).reshape((2, 2)).astype('float')\n",
    "\n",
    "    @property\n",
    "    def F(self):\n",
    "        return np.array([[1, self.dt],\n",
    "                         [-self.g/self.l*np.cos(self.x[0].item())*self.dt, 1]])\n",
    "\n",
    "    def predict(self):\n",
    "        self.x[0] += 0.5 * self.x[1] * self.dt\n",
    "        self.x[1] += -self.g / self.l * np.sin(self.x[0]) * self.dt\n",
    "        self.x[0] += 0.5 * self.x[1] * self.dt\n",
    "        self.P = self.F.dot(self.P).dot(self.F.T) + self.Q\n",
    "\n",
    "    def update(self, y):\n",
    "        y = np.asarray(y).reshape((-1, 1))\n",
    "        S = self.H.dot(self.P).dot(self.H.T) + self.R\n",
    "        K = self.P.dot(self.H.T).dot(np.linalg.inv(S))\n",
    "        self.x = self.x + K.dot(y - self.H.dot(self.x))\n",
    "        self.P = (np.eye(self.P.shape[0]) - K.dot(self.H)).dot(self.P)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.x.squeeze(), self.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_system = StochasticPendulum(g, l, dt, sigma_w, sigma_v, x0)\n",
    "extended_kf = ExtendedKalmanFilter(g, l, dt, sigma_w, sigma_v, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 300\n",
    "\n",
    "x_true = np.zeros((num_steps+1, 2))\n",
    "y_meas = np.zeros(num_steps)\n",
    "x_hat = np.zeros((num_steps, 2))\n",
    "P_hat = np.zeros((num_steps, 2, 2))\n",
    "\n",
    "x_true[0] = real_system.get_state()\n",
    "for k in range(num_steps):\n",
    "    real_system.evolve()\n",
    "    x_true[k+1] = real_system.get_state()\n",
    "    y_meas[k] = real_system.observe()\n",
    "    extended_kf.predict()\n",
    "    extended_kf.update(y_meas[k])\n",
    "    x_hat[k], P_hat[k] = extended_kf.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 4))\n",
    "\n",
    "axes[0].plot(np.arange(num_steps+1), x_true[:,0], color=plt.cm.Dark2(2), zorder=3, label='true process')\n",
    "axes[0].plot(np.arange(1, num_steps+1), y_meas, linestyle='', marker='o', markersize=5,\n",
    "             color=plt.cm.Dark2(1), alpha=0.2, clip_on=False, zorder=2, label='observations')\n",
    "axes[0].plot(np.arange(1, num_steps+1), x_hat[:,0], linestyle='--',\n",
    "             color=plt.cm.Dark2(3), zorder=5, label='EKF estimate')\n",
    "axes[0].fill_between(np.arange(1, num_steps+1), x_hat[:,0]-np.sqrt(P_hat[:,0,0]), x_hat[:,0]+np.sqrt(P_hat[:,0,0]),\n",
    "                     color=plt.cm.Dark2(7), alpha=0.2, zorder=4, label='EKF uncertainty')\n",
    "axes[0].set(xlabel='time step', ylabel='angular position', xlim=(0, num_steps))\n",
    "\n",
    "axes[1].plot(np.arange(num_steps+1), x_true[:,1], color=plt.cm.Dark2(2), zorder=3, label='true process')\n",
    "axes[1].plot(np.arange(1, num_steps+1), x_hat[:,1], linestyle='--',\n",
    "             color=plt.cm.Dark2(3), zorder=5, label='EKF estimate')\n",
    "axes[1].fill_between(np.arange(1, num_steps+1), x_hat[:,1]-np.sqrt(P_hat[:,1,1]), x_hat[:,1]+np.sqrt(P_hat[:,1,1]),\n",
    "                     color=plt.cm.Dark2(7), alpha=0.2, zorder=4, label='EKF uncertainty')\n",
    "axes[1].set(xlabel='time step', ylabel='angular velocity', xlim=(0, num_steps))\n",
    "\n",
    "for ax in axes.ravel().tolist():\n",
    "    ax.legend()\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "fig.suptitle('extended Kalman filter')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-western",
   "metadata": {},
   "source": [
    "### Particle filter\n",
    "\n",
    "We adapt a PF for the state estimation of the noisy pendulum under stochastic excitations. Specifically, a bootstrap PF is used, since it is one of the most intuitive and easy-to-implement SIR schemes. The auxiliary importance density is basically given by the prior model of the state evolution; resampling is done after every step. Again, the state estimates and their uncertainties are compared to the actual values of the simulated process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BootstrapParticleFilter():\n",
    "    '''Particle filter algorithm with a bootstrap scheme.'''\n",
    "\n",
    "    def __init__(self, g, l, dt,\n",
    "                 sigma_w, sigma_v,\n",
    "                 x0=None, P0=None,\n",
    "                 num_particles=100):\n",
    "        self.g = g\n",
    "        self.l = l\n",
    "        self.dt = dt\n",
    "\n",
    "        self.Q = np.array([[(dt**3)/3, (dt**2)/2],\n",
    "                           [(dt**2)/2, dt]]) * sigma_w**2\n",
    "\n",
    "        self.sigma_v = sigma_v\n",
    "\n",
    "        if x0 is None:\n",
    "            x0 = np.zeros((2,))\n",
    "        else:\n",
    "            x0 = np.array(x0).reshape((2,)).astype('float')\n",
    "\n",
    "        if P0 is None:\n",
    "            P0 = np.eye(2)\n",
    "        else:\n",
    "            P0 = np.array(P0).reshape((2, 2)).astype('float')\n",
    "\n",
    "        self.samples = multivariate_normal(mean=x0, cov=P0).rvs(size=num_particles)\n",
    "        self.weights = np.ones((num_particles,)) / num_particles\n",
    "\n",
    "    @property\n",
    "    def num_particles(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _sample(self):\n",
    "        self.samples[:,0] += 0.5 * self.samples[:,1] * self.dt\n",
    "        self.samples[:,1] += -self.g / self.l * np.sin(self.samples[:,0]) * self.dt\n",
    "        self.samples[:,0] += 0.5 * self.samples[:,1] * self.dt\n",
    "        self.samples += multivariate_normal(cov=self.Q).rvs(size=self.num_particles)\n",
    "\n",
    "    def _reweight(self, y):\n",
    "        self.weights *= norm(loc=self.samples[:,0], scale=self.sigma_v).pdf(y)\n",
    "        self.weights /= self.weights.sum()\n",
    "\n",
    "    def update(self, y):\n",
    "        self._sample()\n",
    "        self._reweight(y)\n",
    "\n",
    "    def resample(self, num_particles=None):\n",
    "        if num_particles is None:\n",
    "            num_particles = self.num_particles\n",
    "        random_ids = np.random.choice(\n",
    "            self.num_particles,\n",
    "            size=num_particles,\n",
    "            replace=True,\n",
    "            p=self.weights\n",
    "        )\n",
    "        self.samples = self.samples[random_ids]\n",
    "        self.weights = np.ones((num_particles,)) / num_particles\n",
    "\n",
    "    def estimate(self):\n",
    "        mean = np.average(self.samples, axis=0, weights=self.weights)\n",
    "        cov = np.diag(np.average((self.samples-mean)**2, axis=0, weights=self.weights))\n",
    "        cov[1,0] = cov[0,1] = np.average((self.samples[:,0]-mean[0])*(self.samples[:,1]-mean[1]),\n",
    "                                         axis=0, weights=self.weights)\n",
    "        return mean, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_system = StochasticPendulum(g, l, dt, sigma_w, sigma_v, x0)\n",
    "bootstrap_pf = BootstrapParticleFilter(g, l, dt, sigma_w, sigma_v, x0, num_particles=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 300\n",
    "\n",
    "x_true = np.zeros((num_steps+1, 2))\n",
    "y_meas = np.zeros(num_steps)\n",
    "x_hat = np.zeros((num_steps, 2))\n",
    "P_hat = np.zeros((num_steps, 2, 2))\n",
    "\n",
    "x_true[0] = real_system.get_state()\n",
    "for k in range(num_steps):\n",
    "    real_system.evolve()\n",
    "    x_true[k+1] = real_system.get_state()\n",
    "    y_meas[k] = real_system.observe()\n",
    "    bootstrap_pf.update(y_meas[k])\n",
    "    x_hat[k], P_hat[k] = bootstrap_pf.estimate()\n",
    "    bootstrap_pf.resample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11, 4))\n",
    "\n",
    "axes[0].plot(np.arange(num_steps+1), x_true[:,0], color=plt.cm.Dark2(2), zorder=3, label='true process')\n",
    "axes[0].plot(np.arange(1, num_steps+1), y_meas, linestyle='', marker='o', markersize=5,\n",
    "             color=plt.cm.Dark2(1), alpha=0.2, clip_on=False, zorder=2, label='observations')\n",
    "axes[0].plot(np.arange(1, num_steps+1), x_hat[:,0], linestyle='--',\n",
    "             color=plt.cm.Dark2(3), zorder=5, label='PF estimate')\n",
    "axes[0].fill_between(np.arange(1, num_steps+1), x_hat[:,0]-np.sqrt(P_hat[:,0,0]), x_hat[:,0]+np.sqrt(P_hat[:,0,0]),\n",
    "                     color=plt.cm.Dark2(7), alpha=0.2, zorder=4, label='PF uncertainty')\n",
    "axes[0].set(xlabel='time step', ylabel='angular position', xlim=(0, num_steps))\n",
    "\n",
    "axes[1].plot(np.arange(num_steps+1), x_true[:,1], color=plt.cm.Dark2(2), zorder=3, label='true process')\n",
    "axes[1].plot(np.arange(1, num_steps+1), x_hat[:,1], linestyle='--',\n",
    "             color=plt.cm.Dark2(3), zorder=5, label='PF estimate')\n",
    "axes[1].fill_between(np.arange(1, num_steps+1), x_hat[:,1]-np.sqrt(P_hat[:,1,1]), x_hat[:,1]+np.sqrt(P_hat[:,1,1]),\n",
    "                     color=plt.cm.Dark2(7), alpha=0.2, zorder=4, label='PF uncertainty')\n",
    "axes[1].set(xlabel='time step', ylabel='angular velocity', xlim=(0, num_steps))\n",
    "\n",
    "for ax in axes.ravel().tolist():\n",
    "    ax.legend()\n",
    "    ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "fig.suptitle('bootstrap particle filter')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
